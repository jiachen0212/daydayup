一、计算机视觉: 

1. yolo系的优势:
快, pipline轻, 实时性不错, 工业上部署也较成熟.

2. yolox:
	1. 添加EMA权值更新, Cosine学习率机制等训练技巧
	2. 使用IOU loss做reg优化, BCE loss训cls
	3. 添加RandomHorizontalFlip, ColorJitter, 多尺度数据增广, 移除RandomResizedCrop


3. 实现focal loss: -alpha*(torch.pow((1-probs), self.gamma))*log_p
ce loss: loss = -log(pt)
focal loss: loss = -alpha*(1-pt)^gamma * log(pt)


4. attention如何计算, self attention除以根号k的原因:
attention计算: kq三个向量做"相关性"计算,再加权到v上.
self attention除以根号k: 主要为了scale, 原因: 
1. 内积值可能很大, 不normalize下的话计算量会较大; 2. 内积值太大送给softmax,可能导致梯度很小. 
[softmax的梯度函数: (a: S(xi)*(1-S(xi) or b: (-S(xi)*S(xj))). 先看a, xi太大了, S(xi)趋近于1, 则a值趋近0; 再看b, 当各xi,j间方差很大, 则xi与xj要么一个0要么一个1, b值仍会趋于0.]. 
So除以sqrt(dk), 把内积方差值化为1,就可解决上面俩"隐患".


5. 用ln（layer norm）的原因:
因为样本长度不一致, 用BN需要把各个句子padding到size一致, 添加太多冗余信息.
另外, 一个句子在内部做统计量抓取就可了, 句子本身就有独立性和分布特性, 无需跨越不用样本(尤其样本间差异很大) 

6. swin transformer: 
1. 在局部小window内做attention, 计算量从图像分辨率的平方倍优化至线性倍; 2. 使用shifted层级结构+patch merging, 实现多层级多感受野目的(第二点也是为分割,检测等密集型预测任务做的必须行优化).

7. transformer编码顺序信息: 
用sin, cosin实现每个句子中词index等价线性, 不受句子长度影响. 4个词的句子和10个词的句子, index=2处的编码力度一致.